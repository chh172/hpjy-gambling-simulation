# coding=utf-8
'''
File Header：
@Author: Chuhuan Huang
Contacts: chh172@ucsd.edu/ chuhuan1118@163.com/ chuhuanh@usc.edu
Description:
This file formalized the Gashapon Machine rules by transferring to Finite Markov Decision Process,
where the State Space S, Action Space A, state transition probability function and rewards function are calculated by
hand.
The policy function is where reinforcement learning kicks in: optimal policy is generated by Value Iteration
'''

# Finite Markov Decision Process: element preparation

# State Space S: s = (norm, count, sign)
S = {(-1, 0, 1),
     (0, 0, 0), (0, 0, 1),
     (1, 0, 0), (1, 1, 0), (1, 2, 0), (1, 0, 1),
     (2, 0, 0), (2, 1, 0), (2, 2, 0), (2, 0, 1),
     (3, 0, 0), (3, 1, 0), (3, 2, 0), (3, 0, 1),
     (4, 0, 0), (4, 1, 0), (4, 2, 0), (4, 0, 1),
     (5, 0, 0), (5, 1, 0), (5, 2, 0), (5, 0, 1),
     (6, 0, 0), (6, 1, 0), (6, 2, 0),
     (7, 0, 0),
     (8, 0, 0),
     (9, 0, 0)}

# 8,9 are hidden/爆星, -1为碎星, zero point is the start state and the terminal state
# the first dimension define the norm of the state,
# the second dimension counts the times of protected trial /保护追加，
# the third signifies its hometown
# that is, it is a step-up/0 (coming from "lower" state) or step-down/1(coming from higher state)
# state with sign 1 or with norm greater than or equal to 7 is an enforced pre-terminal state
# that is, their only acceptable action is 4/collect-rewards-and-go-back-to-origin
# others are voluntary pre-terminal state

A = {1, 2, 3, 4}
# Action Space
# start = 1, unprotected trial/平A = 2, protected trial/保护 = 3,
# collect & exit / collect-rewards-and-go-back-to-origin/领取奖励、结束本轮游戏并回到原点 = 4

unit = float(650 / 2880.0)
# value of a single coin in hpjy or serve as exchange rate of coin to RMB
# in fact unit is a function of time that with time elapsing price of Aston Martin in HPJY varies
# this part should be included in the last
# for now for the purpose of learning deep reinforcement learning algo, we fix the unit as constant

COST_ARR = [6, 6, 17, 51, 153, 430, 827, 0, 0, 0, 0]
# cost of step-up in action 3 /protected trial/保护追加
# defined in RMB

REWARD_ARR = [0, 12, 36, 108, 320, 960, 2880, 8640, 8640, 8640]
# reward of each x-axis node if action 4 taken, 0 serves as an offset
# defined in coin

Be_DIST = [0.2, 0.8]
# success or failure

STEP_UP_DIST = [0, 0.82, 0.17, 0.01, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
# step-up probability distribution, 0 serves as an offset, up by 1, 2, 3 stars respectively

STEP_DOWN_DIST = [0, 0.75, 0.25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
# step-down probability distribution, 0 serves as an offset, down by 1, 2 stars respectively

threshold = 0.001
# threshold in policy generation
Value = dict()
# the dictionary that represent the value function
X = set()
# the buffer set


# state transition probability function p(s'|s, a), defined in S x S x A ---> [0,1]
# the probability of state s transit to s' following the action a, sp - s prime state, s - state, a - action
def prob(sp, s, a):
    """
    :rtype: float
    """
    if sp not in S or s not in S or a not in A:
        # filter out invalid input variables
        return 0.0
    # starting position has only one acceptable action : start / action a = 1
    if s == (0, 0, 0):
        return STEP_UP_DIST[sp[0]] * int(0 <= sp[0] <= 3 and sp[1] == 0 == sp[2] and a == 1)
    # pre-terminal state has only one acceptable action : collect&exit / action a = 4
    elif s == (7, 0, 0) or s == (8, 0, 0) or s == (9, 0, 0) or s[2] == 1:
        return int(sp == (0, 0, 0) and a == 4)
    # general cases
    # sorting by action
    else:
        # step-up after-state has characteristics : (x, 0, 0) that is second and third dimension filled with 0
        # step-down after-state has characteristics : (x, 0, 1)
        d1 = sp[0] - s[0]
        d2 = sp[1] - s[1]
        # 平A in s state
        if a == 2:
            return Be_DIST[0] * STEP_UP_DIST[d1] * int(0 < d1 <= 3 and sp[1] == 0 == sp[2]) \
                   + Be_DIST[1] * STEP_DOWN_DIST[-d1] * int(d1 < 0 and sp[1] == 0 and sp[2] == 1)

        # a == 3/ protected trial /保护追加 in s state
        # protected trial is impossible to fail. Therefore third dimension = 0 has to be satisfied.
        # note that the count of protection times needs to be re-defaulted/re-calibrated/set-to-zero after each
        # step-up. Therefore second dimension == 0 has to be satisfied.

        # first part of the calculation formula is the case where player is
        # about to take the third time action 3/ protected trial, which, by rule, is secured to be a successful step-up
        # second and the third part consider the general protection cases, second considers the case protection
        # failed to step-up while the third succeed to step-up
        elif a == 3:
            return STEP_UP_DIST[d1] * int(s[1] == 2 and 0 < d1 <= 3 and sp[1] == 0 == sp[2]) + \
                   Be_DIST[1] * int(0 <= s[1] < 2 and d1 == 0 == sp[2] and d2 == 1) + \
                   Be_DIST[0] * STEP_UP_DIST[d1] * int(0 <= s[1] < 2 and 0 < d1 <= 3 and sp[1] == 0 == sp[2])
        # 直接领取
        elif a == 4:
            return int(sp == (0, 0, 0))
        else:
            return 0.00


# reward function: reward of action a from state s to state sp, defined in S x S x A ---> R (set of Real numbers)
# notice that there is no instant reward for step-up. that is,prize are postponed till exit/action 4
# while step-down is followed by a solely-acceptable state represented in third dimension,
# immediate reward (action 4) and going-back to origin
# e.g. (6, 1, 0) -> (5, 0, 1) -> (0, 0, 0)
def rewards(sp, s, a):
    """

    :rtype: float
    """
    # in this function by using filter above we only need to consider possible rewards
    # impossible ones have been filtered
    if prob(sp, s, a) == 0:
        return 0.0
    # start the game with an entry fee
    if s == (0, 0, 0):
        return - COST_ARR[0]
    # subsistence allowance case/ 碎星吃低保
    # the only two cases we need back trace
    elif s == (0, 0, 1):
        return 4 * unit
    elif s == (-1, 0, 1):
        return 2 * unit
    else:
        # a == 2 平A free of charge
        # a == 3 保护追加 cost is defined in COST_ARR
        # a == 4 直接领取 回到起点
        return 0 * unit * int(a == 2) - COST_ARR[s[0]] * int(a == 3) + REWARD_ARR[s[0]] * unit * int(a == 4)


# Optimized Policy generation using Value Iteration
# policy is a function defined as S ---> A
# here we applied Value Iteration to acquire a deterministic policy that optimize the total reward
# we will consider stochastic policy generation after this part is finished
def policy(state):
    value_function_init()
    value_iteration()
    action = 0
    if state == (0, 0, 0):
        return 1
    for a in A:
        if q(state, a) == max_q(state):
            action = a
            break
    return action


# initialize the value function, which here is implemented as dictionary
def value_function_init():
    for s in S:
        Value[s] = 0.0
    return


# Value Iteration
# Reinforcement Learning: An introduction, 2nd edition, Richard S. Sutton and Andrew G. Barto
# Page 83
def value_iteration():
    delta = 10
    while delta >= threshold:
        delta = 0
        for s in S - {(0, 0, 0)}:
            v = Value[s]
            Value[s] = max_q(s)
            delta = max(delta, abs(Value[s] - v))
    return


# state-action value function/ q function
# represents the expected reward from state s, by action a
def q(s, a):
    M = 0.0
    for sp in S:
        M = M + prob(sp, s, a) * (rewards(sp, s, a) + Value[sp])
    return M


# maximum selector of q function
def max_q(s):
    X.clear()
    for a in A:
        X.add(q(s, a))
    return max(X)
